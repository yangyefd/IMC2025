import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
import joblib

# 1. 创建 StandardScaler 并手动设置参数
scaler = StandardScaler(with_mean=True, with_std=True)
scaler.mean_ = np.array([
    829.4424189392835,
    0.4626679317400709,
    0.2209706720597929,
    0.11671072116598609,
    0.9335745914583636,
    0.4384380924584123,
    0.2830991288563752,
    0.6265973379853325,
    5.813578911683714,
    527.6802372895824,

    416.2936848519031,
531.5041606640901,
419.1911773772301,
181.41560475436296,
136.19421817700404,
180.94831382838786,
139.08740103481844,
227.663839388754,
127.84895928246934,
69.73922034175604,

43.19410050607848,
753815.5220467647,
766219.6351844611,
0.0033081854580957895,
0.003315456698003465,
1.4902675108731103,
1.2001573933812164,
0.5891597721232512,
0.7193758987284533,
0.0390867199379869,

0.1707047590600846,
0.6052589125161486,
0.19820791390042336,
65.81408293228331,
826.4147578881676,
338.703556334571,
0.46171373375820335,
0.10084869694746001,
65.47258276214087,
832.4700807903995,

343.2078089746918,
0.4636221278394341,
0.1014112738981868,
1.003032715832223,
0.9969672841677774,
1.0013296560155072,
0.9986703453422294
    ])  # 从 OCR 获取的均值
scaler.scale_ = np.array([
    592.1561210445249,
0.1690123737144635,
0.0591563566393213,
0.061172235205322056,
0.13270851898985625,
0.19195907420063338,
0.12417649707563053,
0.24836794023912902,
1.3786757485248322,
588.1965471404112,
    
    357.1656290075796,
599.4036534605154,
356.6176544602219,
185.47712346831415,
168.78926389995917,
192.739371541994,
179.71212801946908,
430.3102835527167,
157.3227930672651,
135.3702143328051,

75.223814307941,
2470142.0401964136,
2505227.4505660674,
0.0036640561399569546,
0.003592226226316973,
2.0022871478378486,
0.8820454217502077,
2.836033838321807,
0.26020814597337905,
0.02279907416683736,

0.016856319822204056,
0.806521379154737,
2.3978020298156735,
23.37867542533502,
469.5018449431429,
115.2336204203788,
0.12060436309850191,
0.030828725510073488,
23.123974776278867,
471.0572754850959,

114.72569121991665,
0.12178726852209118,
0.031526605730450664,
0.8975297897410571,
0.8957104472358794,
0.24226090947400394,
0.24349423461853567,
])  # 从 OCR 获取的标准差

scaler.var_ = scaler.scale_ ** 2  # var_ 是 scale_ 的平方，scikit-learn 需要

# 2. 创建 LogisticRegression 并设置参数
logreg = LogisticRegression(
    penalty='l2',
    C=1.0,
    solver='liblinear',
    max_iter=10000,
    class_weight='none',
    random_state=42,
    fit_intercept=True
)

# 手动设置训练后的参数
logreg.coef_ = np.array([[
    0.297750611096025,
    0.014805356985023652,
    0.3720472076554128,
    -0.18566668268753259,
    0.3715290075886357,
    -0.038400839561295176,
    0.06573156461443709,
    0.0,
    -0.5218385670125193,
    0.0,

    -0.012747344975060556,
    -0.17692691547491662,
    -0.2968387554608526,
    0.027690555028751774,
    0.09390909468015794,
    0.1239869741782558,
    0.2423434810862141,
    0.12158027512603944,
    -0.19309923659808192,
    -0.053808355218931535,

    -0.20328908541700064,
    0.24502140867009378,
    -0.08345713873857236,
    0.06434111262270115,
    0.8683362887216548,
    -0.270163931236231,
    -0.09533968239411439,
    0.025372385535341543,
    -0.00222601763729691,
    -0.15352353503102809,

    0.13710901513489607,
    0.0,
    -0.21879545865312952,
    -0.4705493110958642,
    -0.03905789322262896,
    0.02221319334550299,
    0.7470221397327154,
    -0.3380267267644374,
    -0.4014146628835867,
    1.0238208533363236,

    -0.26334525736245235,
    -0.6227489596906995,
    -0.057790350008016696,
    0.30543128716519846,
    -0.20452581032963307,
    0.05686975628513701,
    -0.2638579378660636
    ]])  # 形状为 [n_classes, n_features]
logreg.intercept_ = np.array([3.48449395])  # 形状为 [n_classes]
logreg.classes_ = np.array([0, 1])  # 假设二分类，类标签为 0 和 1
logreg.n_features_in_ = 47  # 特征数量

feature_names = ['num_matches', 'score_mean', 'score_std', 'score_min', 'score_max', 'score_median', 'score_q25', 'score_q75', 'score_top_bottom_ratio', 'kp1_x_mean', 'kp1_y_mean', 'kp2_x_mean', 'kp2_y_mean', 'kp1_x_std', 'kp1_y_std', 'kp2_x_std', 'kp2_y_std', 'kp_x_mean_diff', 'kp_y_mean_diff', 'kp_x_std_diff', 'kp_y_std_diff', 'kp1_area', 'kp2_area', 'kp1_density', 'kp2_density', 'area_ratio', 'distance_ratio_mean', 'distance_ratio_std', 'distance_ratio_consistency', 'desc_sim_mean', 'desc_sim_std', 'score_skew', 'score_kurtosis', 'key1_connections', 'key1_matches_mean', 'key1_matches_std', 'key1_scores_mean', 'key1_scores_std', 'key2_connections', 'key2_matches_mean', 'key2_matches_std', 'key2_scores_mean', 'key2_scores_std', 'key1_matches_ratio', 'key2_matches_ratio', 'key1_scores_ratio', 'key2_scores_ratio']

# 4. 保存为 pkl 文件
joblib.dump(scaler, './lr_model/scaler.pkl')
joblib.dump(logreg, './lr_model/lr_model.pkl')
joblib.dump(feature_names, './lr_model/feature_names.pkl')

#检查sum
print("scaler.mean_:", scaler.mean_.sum(),scaler.mean_.shape)
print("scaler.scale_:", scaler.scale_.sum(),scaler.scale_.shape)
print("logreg.coef_:", logreg.coef_.sum(),logreg.coef_.shape)
